import pandas as pd
from datasets import Dataset
from dataset import preprocess
from train import train_model
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast
import numpy as np

# KoBART í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = PreTrainedTokenizerFast.from_pretrained("gogamza/kobart-base-v2")
model = BartForConditionalGeneration.from_pretrained("gogamza/kobart-base-v2")

# 512 í† í° ë‹¨ìœ„ë¡œ ê¸´ ë¬¸ì¥ ë¶„í• í•˜ëŠ” í•¨ìˆ˜
def split_long_text(row, max_tokens=512):
    source = row["source"]
    target = row["target"]
    tokens = tokenizer.encode(source)

    if len(tokens) <= max_tokens:
        return [dict(row)]  # âœ… Series â†’ dictë¡œ ë³€í™˜í•´ì„œ ë°˜í™˜

    chunks = []
    for i in range(0, len(tokens), max_tokens):
        chunk = tokenizer.decode(tokens[i:i+max_tokens], skip_special_tokens=True)
        chunks.append({"source": chunk, "target": target})

    return chunks


# ì „ì²´ ë°ì´í„°í”„ë ˆì„ì— ë¶„í•  ì ìš©
def expand_dataframe(df):
    expanded = []
    for _, row in df.iterrows():
        expanded.extend(split_long_text(row))
    return pd.DataFrame(expanded)

def main():
    print("âœ… main ì‹œì‘ë¨")

    train_df = pd.read_csv("/home/elicer/Project/CSV/train.csv")
    val_df = pd.read_csv("/home/elicer/Project/CSV/val.csv")
    print("âœ… CSV ë¡œë”© ì™„ë£Œ")

    train_df = train_df.dropna(subset=["source", "target"]).reset_index(drop=True)
    val_df = val_df.dropna(subset=["source", "target"]).reset_index(drop=True)
    print("âœ… ê²°ì¸¡ì¹˜ ì œê±° ì™„ë£Œ")

    train_df = expand_dataframe(train_df)
    val_df = expand_dataframe(val_df)
    print("âœ… ë¬¸ì¥ ë¶„í•  ì™„ë£Œ")

    val_target_texts = val_df["target"].tolist()
    target_token_lens = [len(tokenizer.encode(text)) for text in val_target_texts]
    avg_target_length = int(np.mean(target_token_lens))
    generation_limit = avg_target_length + 10
    print(f"âœ… target í‰ê·  ê¸¸ì´: {avg_target_length}, generation_limit: {generation_limit}")

    train_dataset = Dataset.from_pandas(train_df)
    val_dataset = Dataset.from_pandas(val_df)
    tokenized_train = train_dataset.map(preprocess, remove_columns=["source", "target"])
    tokenized_val = val_dataset.map(preprocess, remove_columns=["source", "target"])
    print("âœ… í† í¬ë‚˜ì´ì§• ì™„ë£Œ")

    print("ğŸš€ train_model í˜¸ì¶œ ì‹œì‘")
    train_model(tokenized_train, tokenized_val, generation_limit)
    print("ğŸ‰ train_model í˜¸ì¶œ ì¢…ë£Œ")

if __name__ == "__main__":
    main()

