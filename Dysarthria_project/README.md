# Ko_BART vs Ko_T5
## 구음장애인의 의사소통 지원을 위한 발화 재구성 시스템용 언어 모델 선정 평가

**본 연구는 구음장애인의 비정형 발화를 보다 명확하고 자연스러운 문장으로 변환하기 위해, 한국어 대형 언어 모델 기반의 재구성 성능을 비교·분석한 연구**

> KoBART와 Ko-T5 모델을 대상으로, 자연스러움·정확성·문맥 일관성 측면의 정성·정량 평가 진행

# CONTENTS

1. 연구 배경 및 목적
2. 데이터셋 구성 및 전처리 
3. 모델 개요 및 구조 
4. 실험 결과 및 분석 (결론)
5. 기대효과
6. 향후 연구 방향
7. 참고 자료 

---

# 연구 배경 및 목적

1️⃣ 구음장애: 말소리를 만드는 운동기관(혀, 입술, 성대 등)의 마비 또는 운동조절 능력 저하로 인해 명확한 발화가 어려운 언어장애

2️⃣ 의사소통 단절 :arrow_right: 우울감, 사회 고립, 정보 접근의 어려움 초래

3️⃣ 대안 필요성: 기존 STT 시스템은 구음장애인의 발화를 제대로 인식하지 못함 :arrow_right: 후처리/보정 필요

4️⃣ 연구 목적:
  - Whisper로 변환된 STT 문장을 자연스럽고 정형화된 문장으로 복원
  - 이를 위한 한국어 LLM(KoBART, KoT5)의 성능 비교 및 적합 모델 선정

---

# 데이터셋 구성 및 전처리

## 🔹 데이터 출처 및 구성

- AI Hub 구음장애 음성 인식 데이터셋 활용
- 총 **486개 샘플 확보**
  - 학습: 339개
  - 검증: 98개
  - 테스트: 49개

##  :abcd: 전처리 절차
 
1. **VAD (Voice Activity Detection)**: 비음성 구간 제거 :speaker:
2. **STT 처리**: Whisper 모델로 음성을 텍스트로 변환 :sound:
3. **정제 자동화**:
   - 불필요한 반복어, 외래어, 특수문자 제거
   - 1:1 Mapping 구조 정립 (STT → 정답 문장)
   - CER > 0.5 또는 WER > 0.7 샘플 제거
4. **토큰 수 기준 정렬**:
   - KoBART: 256 토큰
   - Ko-T5: 512 토큰 기준으로 문장 구성

---

## 🤖 모델 개요 및 구조

| 모델         | 구조                 | 학습 방식                                            | 입력 처리                      | 특징                         |
| ---------- | ------------------ | ------------------------------------------------ | -------------------------- | -------------------------- |
| **KoBART** | BART 기반 인코더-디코더 구조 | 노이징 복원 학습 (Text Infilling, Sentence Permutation) | 문장 단위 입력, 마스킹 후 복원         | 문맥 재구성에 강하고, 문법적 자연스러움 우수  |
| **Ko-T5**  | T5 기반 텍스트-투-텍스트 구조 | Prefix-based pretraining (Span Corruption)       | 텍스트 전처리 없이 문자열 전체를 태스크로 명시 | 경량화, 범용성 우수. 다양한 태스크 적용 가능 |

**KoBART**
- 인코더-디코더 기반 Seq2Seq 구조

- 학습 시 텍스트 일부를 마스킹하고, 이를 원래 문장으로 복원하는 방식

- 문장 복원 능력과 문법적 유창성에서 뛰어나며, 실제 의사소통에서 중요한 자연스러운 문장 구성에 유리

- 한국어 뉴스, 위키 기반으로 학습되어 정제된 언어 표현 생성에 강점


**KoT5**

- 모든 NLP 작업을 "질문: 내용" → "정답" 형태로 일반화

- 예: "자연어 문장 정제: 강아지가 공 문 다음에 주인을 바라본다" → "강아지가 공을 물고 주인을 바라본다"

- Pre-finetuning 구조 덕분에 다양한 태스크에 유연하게 적용 가능

---

# 실험 결과 및 분석

| 모델 | ROUGE-L | BLEURT | BERTScore |
|------|---------|--------|-----------|
| **KoBART** | 0.855 | **0.787** | 0.9358 |
| **Ko-T5**   | **0.886** | 0.765 | **0.963** |

- **KoT5**는 구조 유사성 및 문맥 일치 측면(ROUGE-L, BERTScore)에서 우세
- **KoBART**는 자연스러움과 의미 정밀성(BLEURT)에서 더 강점

**결론**:  
:arrow_forward: 자연스러운 문장 복원을 더 중요시하는 본 과제에서는 **KoBART**가 보다 적합한 모델로 판단됨

---

# 기대 효과

- **의사소통 정확도 향상**  
  의미가 왜곡된 비정형 발화를 자연스럽고 명확하게 재구성함으로써,  
  사용자 발화의 전달력과 의사소통의 정확도를 개선

-  **심리적 부담 완화**  
  반복적 설명이나 오해로 인한 좌절, 소통 불안 등을 완화하여  
  사용자 자존감 및 심리적 안정감 증진

-  **사회적 고립 해소**  
  원활한 소통을 가능케 함으로써, 교육·직장·가정 등 다양한 사회적 관계에의 참여 확대

-  **AI 기반 맞춤형 언어 지원 가능성 검증**  
  구음장애 발화 데이터를 기반으로 한 LLM fine-tuning 구조를 제시함으로써,  
  향후 다양한 장애군 대상의 **디지털 헬스케어 시스템 기반 기술로 확장** 가능

-  **보조공학 기술과의 연계 가능성**  
  TTS(Text-to-Speech), AAC 기기 등과의 결합을 통해  
  실시간 음성 출력형 의사소통 보조 시스템으로의 발전 기반 마련

---

# 향후 연구 방향

- 구음장애인의 발화 특성에 최적화된 특화 데이터셋 구성
- 일상 대화 환경을 반영한 잡음 포함 음성 학습
- 실시간 양방향 통신 시스템으로 확장 가능성 확보 (예: TTS 연계 등)
- 향후 복수 장애유형 대응 및 챗봇·앱과의 연동까지 고려

---

# 참고자료

1. OpenAI – Whisper: https://openai.com/index/whisper/  
2. BART: https://arxiv.org/abs/1910.13461  
3. T5: https://arxiv.org/abs/1910.10683  
4. BLEURT: https://arxiv.org/abs/2004.04696  
5. BERTScore: https://arxiv.org/abs/1904.09675  
6. ROUGE: https://aclanthology.org/W04-1013/  
7. 구음장애 정의: https://www.peoplepower21.org/welfarenow/1982302  
8. 관련 보도: https://m.health.chosun.com/svc/news_view.html?contid=2024101802413  



